{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install tinydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axis import Axis\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tinydb import TinyDB, Query\n",
    "from tinydb.storages import MemoryStorage\n",
    "from tinydb.table import Table\n",
    "\n",
    "from kyle.evaluation.reliabilities import (\n",
    "    expected_calibration_error,\n",
    "    class_wise_expected_calibration_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: RF and MLP on Synthetic Data\n",
    "\n",
    "Here the sample size is increased by simply including more synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 15000\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 20\n",
    "n_informative = 7\n",
    "n_redundant = 10\n",
    "\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_classes=n_classes,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, calibration set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"mlp\": MLPClassifier(hidden_layer_sizes=(30, 30, 20), max_iter=500),\n",
    "    \"rf\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "predicted_confs = {}\n",
    "\n",
    "for model_name, model in MODELS.items():\n",
    "    print(f\"Fitting {model_name} on {len(X_train)} samples.\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    confs = model.predict_proba(X_test)\n",
    "    predicted_confs[model_name] = confs\n",
    "    y_pred = confs.argmax(axis=1)\n",
    "    model_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test accuracy of {model_name}: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency resampling to get calibrated classifiers\n",
    "\n",
    "calibrated_y_true = {\n",
    "    \"mlp\": np.zeros(len(X_test)),\n",
    "    \"rf\": np.zeros(len(X_test)),\n",
    "}\n",
    "for model, confs in predicted_confs.items():\n",
    "    for i, conf in enumerate(confs):\n",
    "        calibrated_y_true[model][i] = np.random.choice(n_classes, p=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "    \"ECE\": expected_calibration_error,\n",
    "    \"cwECE\": class_wise_expected_calibration_error,\n",
    "}\n",
    "\n",
    "\n",
    "def get_scores(\n",
    "    evaluation_set_size: int,\n",
    "    num_samples: int,\n",
    "    model: str,\n",
    "    metric: str,\n",
    "    consistency_resampling=False,\n",
    "):\n",
    "    results = []\n",
    "    for _ in range(num_samples):\n",
    "        sample_indices = np.random.choice(\n",
    "            len(X_test), evaluation_set_size, replace=False\n",
    "        )\n",
    "        confs = predicted_confs[model][sample_indices]\n",
    "        if not consistency_resampling:\n",
    "            y_true = y_test[sample_indices]\n",
    "        else:\n",
    "            y_true = calibrated_y_true[model][sample_indices]\n",
    "        score = METRICS[metric](y_true, confs)\n",
    "        results.append(score)\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetricEvaluation:\n",
    "    model: str\n",
    "    metric: str\n",
    "    n_bins: int\n",
    "    strategy: int\n",
    "    scores: Optional[np.ndarray] = None\n",
    "    set_size: Optional[int] = None\n",
    "    num_samples: Optional[int] = None\n",
    "    consistency_resampling: bool = False\n",
    "\n",
    "    def perform_evaluation(\n",
    "        self, set_size: int, num_samples: int, consistency_resampling=False\n",
    "    ):\n",
    "        self.set_size = set_size\n",
    "        self.num_samples = num_samples\n",
    "        self.consistency_resampling = consistency_resampling\n",
    "        self.scores = get_scores(\n",
    "            set_size,\n",
    "            num_samples,\n",
    "            self.model,\n",
    "            self.metric,\n",
    "            consistency_resampling=consistency_resampling,\n",
    "        )\n",
    "\n",
    "    def mean(self):\n",
    "        self._assert_nonempty()\n",
    "        return self.scores.mean()\n",
    "\n",
    "    def std(self):\n",
    "        self._assert_nonempty()\n",
    "        return self.scores.std()\n",
    "\n",
    "    def _assert_nonempty(self):\n",
    "        if self.scores is None:\n",
    "            raise RuntimeError(\n",
    "                f\"You must run `perform_evaluation` before computing statistics: {self}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all evaluations to an in-memory database\n",
    "n_bins_options = [5, 40]\n",
    "binning_strategy_options = [\"uniform\", \"quantile\"]\n",
    "\n",
    "\n",
    "def save_evaluations_to_db(\n",
    "    set_sizes: List[int], num_samples: int, db: TinyDB, consistency_resampling=False\n",
    "):\n",
    "    for set_size, model, metric, n_bins, strategy in product(\n",
    "        set_sizes, MODELS, METRICS, n_bins_options, binning_strategy_options\n",
    "    ):\n",
    "        metric_evaluation = MetricEvaluation(model, metric, n_bins, strategy)\n",
    "        metric_evaluation.perform_evaluation(\n",
    "            set_size, num_samples, consistency_resampling=consistency_resampling\n",
    "        )\n",
    "        db.insert(metric_evaluation.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customization of tinydb\n",
    "class EvaluationsTable(Table):\n",
    "    def search(self, cond: Query) -> List[MetricEvaluation]:\n",
    "        results = super().search(cond)\n",
    "        return [MetricEvaluation(**eval_dict) for eval_dict in results]\n",
    "\n",
    "\n",
    "TinyDB.table_class = EvaluationsTable\n",
    "TinyDB.default_storage_class = MemoryStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_sizes = range(500, 8000, 500)\n",
    "num_samples = 10\n",
    "db = TinyDB()\n",
    "evalQ = Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_evaluations_to_db(\n",
    "    set_sizes=set_sizes,\n",
    "    num_samples=num_samples,\n",
    "    db=db,\n",
    "    consistency_resampling=False,\n",
    ")\n",
    "\n",
    "save_evaluations_to_db(\n",
    "    set_sizes=set_sizes,\n",
    "    num_samples=num_samples,\n",
    "    db=db,\n",
    "    consistency_resampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(\n",
    "    model: str,\n",
    "    n_bins: int = None,\n",
    "    strategy: str = None,\n",
    "    metric: str = None,\n",
    "    consistency_resampling=False,\n",
    "):\n",
    "    q = evalQ.model == model\n",
    "    q = q & (evalQ.consistency_resampling == consistency_resampling)\n",
    "    if n_bins:\n",
    "        q = q & (evalQ.n_bins == n_bins)\n",
    "    if strategy:\n",
    "        q = q & (evalQ.strategy == strategy)\n",
    "    if metric:\n",
    "        q = q & (evalQ.metric == metric)\n",
    "    return q\n",
    "\n",
    "\n",
    "def get_evaluations(\n",
    "    model: str,\n",
    "    n_bins: int = None,\n",
    "    strategy: str = None,\n",
    "    metric: str = None,\n",
    "    consistency_resampling=False,\n",
    ") -> List[MetricEvaluation]:\n",
    "    evaluations = db.search(\n",
    "        get_query(\n",
    "            model,\n",
    "            n_bins,\n",
    "            strategy,\n",
    "            metric,\n",
    "            consistency_resampling=consistency_resampling,\n",
    "        )\n",
    "    )\n",
    "    return sorted(evaluations, key=lambda ev: ev.set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(\n",
    "    model: str,\n",
    "    n_bins: int,\n",
    "    strategy: str,\n",
    "    metric: str,\n",
    "    consistency_resampling=False,\n",
    "    delta_x=0,\n",
    "    color=0,\n",
    "    ax: Axis = None,\n",
    "):\n",
    "    selected_evaluations = get_evaluations(\n",
    "        model, n_bins, strategy, metric, consistency_resampling\n",
    "    )\n",
    "\n",
    "    selected_set_sizes = np.zeros(len(selected_evaluations))\n",
    "    means = np.zeros(len(selected_evaluations))\n",
    "    stds = np.zeros(len(selected_evaluations))\n",
    "    for i, ev in enumerate(selected_evaluations):\n",
    "        selected_set_sizes[i] = ev.set_size\n",
    "        means[i] = ev.mean()\n",
    "        stds[i] = ev.std()\n",
    "\n",
    "    if isinstance(color, int):\n",
    "        color = f\"C{color}\"\n",
    "    x_values = selected_set_sizes + delta_x\n",
    "    ymin = means - stds\n",
    "    ymax = means + stds\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    title = f\"{metric} for model: {model}\"\n",
    "    if consistency_resampling:\n",
    "        title += f\" (cons-res. labels)\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"sample size\")\n",
    "    ax.plot(x_values, means, \".\", color=color, label=f\"{n_bins} bins, {strategy}\")\n",
    "    ax.vlines(x_values, ymin=ymin, ymax=ymax, color=color, linewidth=2)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_all_convergences(\n",
    "    model: str, metric: str, ax: Axis = None, delta_x=60, consistency_resampling=False\n",
    "):\n",
    "    for i, (n_bins, strategy) in enumerate(\n",
    "        product(n_bins_options, binning_strategy_options)\n",
    "    ):\n",
    "        ax = plot_convergence(\n",
    "            model,\n",
    "            n_bins,\n",
    "            strategy,\n",
    "            metric,\n",
    "            delta_x=i * delta_x,\n",
    "            color=i,\n",
    "            ax=ax,\n",
    "            consistency_resampling=consistency_resampling,\n",
    "        )\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_win_path(path: str):\n",
    "    return path.replace(\"/c/\", \"C:\\\\\").replace(\"/\", \"\\\\\")\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16, 9))\n",
    "\n",
    "plot_all_convergences(\"mlp\", \"ECE\", ax=axs[0, 0])\n",
    "plot_all_convergences(\"rf\", \"ECE\", ax=axs[0, 1])\n",
    "plot_all_convergences(\"mlp\", \"cwECE\", ax=axs[0, 2])\n",
    "plot_all_convergences(\"rf\", \"cwECE\", ax=axs[0, 3])\n",
    "plot_all_convergences(\"mlp\", \"ECE\", consistency_resampling=True, ax=axs[1, 0])\n",
    "plot_all_convergences(\"rf\", \"ECE\", consistency_resampling=True, ax=axs[1, 1])\n",
    "plot_all_convergences(\"mlp\", \"cwECE\", consistency_resampling=True, ax=axs[1, 2])\n",
    "plot_all_convergences(\"rf\", \"cwECE\", consistency_resampling=True, ax=axs[1, 3])\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
