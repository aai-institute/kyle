{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - this cell should be executed only once per session\n",
    "import sys, os\n",
    "\n",
    "# in order to get top level modules and to have paths relative to repo root\n",
    "\n",
    "if os.path.basename(os.getcwd()) != \"notebooks\":\n",
    "    raise Exception(f\"Wrong directory. Did you execute this cell twice?\")\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class-wise and Reduced Calibration Methods\n",
    "\n",
    "In this notebook we demonstrate two new strategies for calibrating probabilistic classifiers. These strategies act\n",
    "as wrappers around any calibration algorithm and therefore are implemented as wrappers. We test the improvements\n",
    "in different calibration errors due to these wrappers where the non-wrapped calibration methods serve as baselines.\n",
    "\n",
    "The tests are performed on random forests trained on two synthetic data sets (balanced and imbalanced) as well as\n",
    "on resnet20 trained on the CIFAR10 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "from kyle.calibration.calibration_methods import *\n",
    "from kyle.evaluation import EvalStats\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_WRAPPERS = {\n",
    "    \"Baseline\": lambda method_factory: method_factory(),\n",
    "    \"Class-wise\": lambda method_factory: ClassWiseCalibration(method_factory),\n",
    "    \"Reduced\": lambda method_factory: ConfidenceReducedCalibration(method_factory()),\n",
    "    \"Class-wise reduced\": lambda method_factory: ClassWiseCalibration(\n",
    "        lambda: ConfidenceReducedCalibration(method_factory())\n",
    "    ),\n",
    "}\n",
    "\n",
    "DEFAULT_CV = 6\n",
    "DEFAULT_BINS = 25\n",
    "\n",
    "ALL_CALIBRATION_METHOD_FACTORIES = (\n",
    "    # TemperatureScaling,\n",
    "    BetaCalibration,\n",
    "    # LogisticCalibration,\n",
    "    IsotonicRegression,\n",
    "    HistogramBinning,\n",
    ")\n",
    "ALL_METRICS = (\n",
    "    \"ECE\",\n",
    "    \"cwECE\",\n",
    ")\n",
    "\n",
    "\n",
    "def compute_score(scaler, confs: np.ndarray, labels: np.ndarray, bins, metric=\"ECE\"):\n",
    "    calibrated_confs = scaler.get_calibrated_confidences(confs)\n",
    "    eval_stats = EvalStats(labels, calibrated_confs)\n",
    "    if metric == \"ECE\":\n",
    "        return eval_stats.expected_calibration_error(n_bins=bins)\n",
    "    elif metric == \"cwECE\":\n",
    "        return eval_stats.class_wise_expected_calibration_error(n_bins=bins)\n",
    "    elif isinstance(metric, int):\n",
    "        return eval_stats.expected_calibration_error(class_label=metric, n_bins=bins)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric {metric}\")\n",
    "\n",
    "\n",
    "def get_scores(scaler, metric, cv, bins, confs, labels):\n",
    "    scoring = lambda *args: compute_score(*args, bins=bins, metric=metric)\n",
    "    return cross_val_score(scaler, confs, labels, scoring=scoring, cv=cv)\n",
    "\n",
    "\n",
    "def plot_scores(wrapper_scores_dict: dict, title=\"\", ax=None, y_lim=None):\n",
    "    labels = wrapper_scores_dict.keys()\n",
    "    scores_collection = wrapper_scores_dict.values()\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        ax = plt.gca()\n",
    "    ax.set_title(title)\n",
    "    ax.boxplot(scores_collection, labels=labels)\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "\n",
    "\n",
    "def evaluate_calibration_wrappers(\n",
    "    method_factory,\n",
    "    confidences,\n",
    "    gt_labels,\n",
    "    wrappers_dict=None,\n",
    "    metric=\"ECE\",\n",
    "    cv=DEFAULT_CV,\n",
    "    method_name=None,\n",
    "    bins=DEFAULT_BINS,\n",
    "    short_description=False,\n",
    "):\n",
    "    if method_name is None:\n",
    "        method_name = method_factory.__name__\n",
    "    if short_description:\n",
    "        description = f\"{method_name}\"\n",
    "    else:\n",
    "        description = (\n",
    "            f\"Evaluating wrappers of {method_name} on metric {metric} with {bins} bins\\n \"\n",
    "            f\"CV with {cv} folds on {len(confidences)} data points.\"\n",
    "        )\n",
    "    if wrappers_dict is None:\n",
    "        wrappers_dict = DEFAULT_WRAPPERS\n",
    "\n",
    "    wrapper_scores_dict = {}\n",
    "    for wrapper_name, wrapper in wrappers_dict.items():\n",
    "        method = wrapper(method_factory)\n",
    "        scores = get_scores(\n",
    "            method, metric, cv=cv, bins=bins, confs=confidences, labels=gt_labels\n",
    "        )\n",
    "        wrapper_scores_dict[wrapper_name] = scores\n",
    "    return wrapper_scores_dict, description\n",
    "\n",
    "\n",
    "# taken such that minimum and maximum are visible in all plots\n",
    "DEFAULT_Y_LIMS_DICT = {\n",
    "    \"ECE\": (0.004, 0.032),\n",
    "    \"cwECE\": (0.005, 0.018),\n",
    "}\n",
    "\n",
    "\n",
    "def perform_default_evaluation(\n",
    "    confidences,\n",
    "    gt_labels,\n",
    "    method_factories=ALL_CALIBRATION_METHOD_FACTORIES,\n",
    "    metrics=ALL_METRICS,\n",
    "):\n",
    "    evaluation_results = defaultdict(list)\n",
    "    for metric in metrics:\n",
    "        print(f\"Creating evaluation for {metric}\")\n",
    "        for method_factory in method_factories:\n",
    "            print(f\"Computing scores for {method_factory.__name__}\", end=\"\\r\")\n",
    "            result = evaluate_calibration_wrappers(\n",
    "                method_factory,\n",
    "                confidences=confidences,\n",
    "                gt_labels=gt_labels,\n",
    "                metric=metric,\n",
    "                short_description=True,\n",
    "            )\n",
    "            evaluation_results[metric].append(result)\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "def plot_default_evaluation_results(\n",
    "    evaluation_results: dict, figsize=(25, 7), y_lims_dict=None, title_addon=None\n",
    "):\n",
    "    if y_lims_dict is None:\n",
    "        y_lims_dict = DEFAULT_Y_LIMS_DICT\n",
    "    ncols = len(list(evaluation_results.values())[0])\n",
    "    for metric, results in evaluation_results.items():\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=figsize)\n",
    "        y_lim = y_lims_dict[metric]\n",
    "        if ncols == 1:  # axes fails to be a list if ncols=1\n",
    "            axes = [axes]\n",
    "        for col, result in zip(axes, results):\n",
    "            wrapper_scores_dict, description = result\n",
    "            plot_scores(wrapper_scores_dict, title=description, ax=col, y_lim=y_lim)\n",
    "\n",
    "        title = f\"Evaluation with {metric} ({DEFAULT_CV} folds; {DEFAULT_BINS} bins)\"\n",
    "        if title_addon is not None:\n",
    "            title += f\"\\n{title_addon}\"\n",
    "        fig.suptitle(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibration_dataset(\n",
    "    n_classes=5,\n",
    "    weights=None,\n",
    "    n_samples=30000,\n",
    "    n_informative=15,\n",
    "    model=RandomForestClassifier(),\n",
    "):\n",
    "    n_dataset_samples = 2 * n_samples\n",
    "    test_size = 0.5\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_dataset_samples,\n",
    "        n_classes=n_classes,\n",
    "        n_informative=n_informative,\n",
    "        weights=weights,\n",
    "    )\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "    train_index, test_index = list(sss.split(X, y))[0]\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    confidences = model.predict_proba(X_test)\n",
    "    y_pred = confidences.argmax(1)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "    return confidences, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "print(f\"Creating balanced dataset\")\n",
    "balanced_confs, balanced_gt = get_calibration_dataset()\n",
    "print(f\"Creating unbalanced dataset\")\n",
    "unbalanced_confs, unbalanced_gt = get_calibration_dataset(\n",
    "    weights=(0.3, 0.1, 0.25, 0.15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating wrappers on a single calibration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_scores_ECE, description = evaluate_calibration_wrappers(\n",
    "    HistogramBinning,\n",
    "    confidences=balanced_confs,\n",
    "    gt_labels=balanced_gt,\n",
    "    metric=\"ECE\",\n",
    "    cv=4,\n",
    ")\n",
    "\n",
    "plot_scores(balanced_scores_ECE, title=description)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_scores_ECE, description = evaluate_calibration_wrappers(\n",
    "    TemperatureScaling,\n",
    "    confidences=unbalanced_confs,\n",
    "    gt_labels=unbalanced_gt,\n",
    "    metric=\"ECE\",\n",
    ")\n",
    "\n",
    "plot_scores(unbalanced_scores_ECE, title=description)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating wrappers on multiple metrics and plotting next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = perform_default_evaluation(\n",
    "    confidences=balanced_confs, gt_labels=balanced_gt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_evaluation_results(eval_results, title_addon=\"Balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_eval_results = perform_default_evaluation(\n",
    "    confidences=unbalanced_confs, gt_labels=unbalanced_gt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_evaluation_results(unbalanced_eval_results, title_addon=\"Unbalanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Resnet\n",
    "\n",
    "Here we will repeat the evaluation of calibration methods on a neural network, specifically\n",
    "on resnet20 trained on the CIFAR10 data set.\n",
    "\n",
    "Important: in order to run the resnet part you will need the packages from `requirements-torch.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kyle.models.resnet import load_weights, resnet20, resnet56\n",
    "from kyle.datasets import get_cifar10_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_resnet = \"resnet20\"\n",
    "\n",
    "weights_file_names = {\n",
    "    \"resnet20\": \"resnet20-12fca82f.th\",\n",
    "    \"resnet56\": \"resnet56-4bfd9763.th\",\n",
    "}\n",
    "\n",
    "models_dict = {\n",
    "    \"resnet20\": resnet20(),\n",
    "    \"resnet56\": resnet56(),\n",
    "}\n",
    "\n",
    "\n",
    "resnet_path = os.path.join(\"data\", \"artifacts\", weights_file_names[selected_resnet])\n",
    "cifar_10_data_path = os.path.join(\"data\", \"raw\", \"cifar10\")\n",
    "logits_save_path = os.path.join(\n",
    "    \"data\", \"processed\", \"cifar10\", f\"logits_{selected_resnet}.npy\"\n",
    ")\n",
    "\n",
    "if not os.path.isfile(resnet_path):\n",
    "    print(\n",
    "        f\"Downloading weights for {selected_resnet} to {os.path.abspath(resnet_path)}\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(resnet_path), exist_ok=True)\n",
    "    url = f\"https://github.com/akamaster/pytorch_resnet_cifar10/raw/master/pretrained_models/{weights_file_names[selected_resnet]}\"\n",
    "    r = requests.get(url)\n",
    "    with open(resnet_path, \"wb\") as file:\n",
    "        file.write(r.content)\n",
    "\n",
    "resnet = models_dict[selected_resnet]\n",
    "load_weights(resnet_path, resnet)\n",
    "resnet.eval()\n",
    "\n",
    "\n",
    "def get_cifar10_confidences():\n",
    "    cifar_10_X, cifar_10_Y = get_cifar10_dataset(cifar_10_data_path)\n",
    "\n",
    "    if os.path.isfile(logits_save_path):\n",
    "        logits = np.load(logits_save_path)\n",
    "    else:\n",
    "        # processing all at once may not fit into ram\n",
    "        batch_boundaries = range(0, len(cifar_10_X) + 1, 1000)\n",
    "\n",
    "        logits = []\n",
    "        for i in range(len(batch_boundaries) - 1):\n",
    "            print(f\"Processing batch {i+1}/{len(batch_boundaries)-1}\", end=\"\\r\")\n",
    "            lower, upper = batch_boundaries[i], batch_boundaries[i + 1]\n",
    "            logits.append(resnet(cifar_10_X[lower:upper]).detach().numpy())\n",
    "\n",
    "        logits = np.vstack(logits)\n",
    "        os.makedirs(os.path.dirname(logits_save_path), exist_ok=True)\n",
    "        np.save(logits_save_path, logits, allow_pickle=False)\n",
    "\n",
    "    confidences = softmax(logits, axis=1)\n",
    "    gt_labels = cifar_10_Y.numpy()\n",
    "    return confidences, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_confs, cifar_gt = get_cifar10_confidences()\n",
    "\n",
    "## Evaluating wrappers on a single calibration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_scores_ECE, description = evaluate_calibration_wrappers(\n",
    "    HistogramBinning, confidences=cifar_confs, gt_labels=cifar_gt, metric=\"ECE\", cv=4\n",
    ")\n",
    "\n",
    "plot_scores(resnet_scores_ECE, title=description)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_scores_ECE, description = evaluate_calibration_wrappers(\n",
    "    TemperatureScaling, confidences=cifar_confs, gt_labels=cifar_gt, metric=\"ECE\", cv=4\n",
    ")\n",
    "\n",
    "plot_scores(resnet_scores_ECE, title=description)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating wrappers on multiple metrics and plotting next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = perform_default_evaluation(\n",
    "    confidences=balanced_confs, gt_labels=balanced_gt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_evaluation_results(\n",
    "    eval_results, title_addon=f\"{selected_resnet} on CIFAR10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
