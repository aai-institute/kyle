{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from kyle.calibration import ModelCalibrator\n",
    "from kyle.models import CalibratableModel\n",
    "from kyle.metrics import ECE\n",
    "from kyle.calibration.calibration_methods import TemperatureScaling\n",
    "from kyle.sampling.fake_clf import DirichletFC\n",
    "from kyle.transformations import ShiftingSimplexAutomorphism\n",
    "from kyle.evaluation import EvalStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is calibration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about how good a machine learning model is, what we (generally) mean to ask is: How accurate is the model? While this is a good enough metric in many cases, we are, in fact, leaving out important information about the model. One such piece of information is concerned with whether the confidence of the model is in line with its accuracy. If it is, we say the model is calibrated.\n",
    "\n",
    "To explain this concept in detail, let's begin with an example. Suppose we want to predict whether a patient has cancer.\n",
    "We can simulate data with two classes i.e. $y \\in \\{0, 1\\}$ where $y=0$ denotes a healthy patient and $y=1$ denotes a patient who has cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=20,\n",
    "        n_informative=7,\n",
    "        n_redundant=10,\n",
    "        n_classes=n_classes,\n",
    "        random_state=42,\n",
    "    )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a neural network on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(50, 50, 50))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make predictions on new samples. Let's see how our model performs on unseen examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f\"Model accuracy: {model_accuracy*100}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems pretty good! One might think our job here is done: After all, the model predicts whether a person has cancer or not with decent accuracy.\n",
    "Unfortunately, accuracy of a model does not tell us the full story. This is so due to the fact that at inference time, \n",
    "for a given sample a model outputs confidence scores for each class. We then take the class with the highest confidence\n",
    "and interpret that as the prediction of the model.\n",
    "\n",
    "This conversion of continuous (probability) to discrete (label) values can hide certain properties of the model.\n",
    "To illustrate this, let's take two models -- $A$ and $B$ -- trained on the same data. Let's further assume they have similar accuracy. Suppose we test both models with 10 healthy samples. $A$ assigns probabilities $(0.49, 0.51)$ to all samples, whereas $B$ assigns $(0.1, 0.9)$. While $A$ & $B$ will be wrong 100% of the time, notice $A$ being much closer to classifying the samples as belonging to the correct class compared to $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our previous example: Imagine that on all examples where the model was $95$% confident that the subject \n",
    "has cancer, it was correct $70$% of the time. Intuitively, it seems there's something not quite right with the model: the model is over-confident in its predictions. This notion is formalized by the concept of calibration. We say a model is (perfectly) calibrated when, for any confidence value $p \\in [0, 1]$, prediction of a class with confidence $p$ is correct with probability $p$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\widehat{y}=y|\\widehat{p}=p) = p \\quad \\forall p \\in [0, 1]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, is our model calibrated? As we can see in the equation above, $\\widehat{p}$ is continuous, which means we cannot compute the equation with finite data. We can, however, develop empirical measures that approximate the true measure of (mis)calibration.\n",
    "\n",
    "One simple way to get an empirical estimate of the model's accuracy and confidence is to discretize the probability space. This is done by slicing $p$ into $K$ equal-sized bins. We can then calculate the accuracy and confidence for each bin:\n",
    "\n",
    "\\begin{equation}\n",
    "accuracy_{B_k} = \\frac{1}{|B_k|} \\sum_{m=1}^{|B_k|}1(\\widehat{p}_m=p_m)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "confidence_{B_k} = \\frac{1}{|B_k|} \\sum_{m=1}^{|B_k|}\\widehat{p}_m\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply calculate the weighted average difference between the accuracy and confidence of the model over all bins:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{k=1}^{K} \\frac{|B_k|}{n} \\Big|\\:accuracy_{B_k} - confidence_{B_k} \\Big|\n",
    "\\end{equation}\n",
    "\n",
    "This is known as the **Expected Calibration Error** $(ECE).$ As can be seen, $ECE=0$ if a model is perfectly calibrated. Let's calculate the $ECE$ for our model with $10$ bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece = ECE(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate uncalibrated predictions\n",
    "uncalibrated_confidences = model.predict_proba(X_test)\n",
    "\n",
    "pre_calibration_ece = ece.compute(uncalibrated_confidences, y_test)\n",
    "\n",
    "f\"ECE before calibration: {pre_calibration_ece}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the extent of miscalibration by plotting the model's confidence *(x-axis)* vs. the ground truth probability *(y-axis)*. For a perfectly calibrated model, the plot should be $y=x$. Let's see how our model fares: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_stats = EvalStats(y_test, uncalibrated_confidences)\n",
    "class_labels = [i for i in range(n_classes)]\n",
    "\n",
    "eval_stats.plot_reliability_curves(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so our model is not calibrated as $ECE>0$. Can we do anything to remedy the situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we can improve the calibration of our model using various techniques. What's more, we don't need to train our model again; many calibration techniques are post-processing methods i.e. operating on the trained model's output confidence scores. The output scores for calibration are typically obtained on a validation set.\n",
    "\n",
    "In `kyle`, we have provided a `CalibratableModel` class which takes a model and, as the name suggests, makes it possible to calibrate that model. By default, we use a technique called [*Temperature scaling*](https://arxiv.org/abs/1706.04599) for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibratable model\n",
    "calibration_method = TemperatureScaling()\n",
    "calibratable_model = CalibratableModel(model, calibration_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a `ModelCalibrator` class which holds the data to calibrate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model calibrator and calibrate model\n",
    "calibrator = ModelCalibrator(X_calibrate=X_test, \n",
    "                             y_calibrate=y_test, \n",
    "                             X_fit=X_train, \n",
    "                             y_fit=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything ready to calibrate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator.calibrate(calibratable_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if calibrating the model improved the $ECE$ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing X_test instead of X_calibrate in predict_proba() to make comparison with pre-calib model clear, \n",
    "# same reasong for y_test in ece.compute()\n",
    "calibrated_confidences = calibratable_model.predict_proba(X_test)\n",
    "\n",
    "post_calibration_ece = ece.compute(calibrated_confidences, y_test)\n",
    "\n",
    "f\"ECE before calibration: {pre_calibration_ece}, ECE after calibration: {post_calibration_ece}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! $ECE$ has improved. Let's also plot a reliability curve to visually confirm the improvement in calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_stats = EvalStats(y_test, calibrated_confidences)\n",
    "\n",
    "eval_stats.plot_reliability_curves(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! We have successfully improved our model's calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-agnostic calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that to evaluate (mis)calibration of a model, we don't require the model itself. Rather, it is sufficient to have the confidence scores predicted by the model. This means we can abstract away the model and generate both the ground truth and confidence scores via sampling processes.\n",
    "\n",
    "In `kyle` we have provided samplers that simulate different kinds of calibration properties. One such sampler is the `DirichletFC` class which provides calibrated ground truth and confidences by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DirichletFC(num_classes=2)\n",
    "\n",
    "# Get 1000 calibrated fake confidence scores\n",
    "calibrated_samples = sampler.get_sample_arrays(1000)\n",
    "ground_truth, confidences = calibrated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the $ECE$ for these samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece.compute(confidences, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, the $ECE>0$, how can we say that the samples are calibrated?\n",
    "\n",
    "As mentioned earlier, we only have finite samples so true miscalibration can only be measured asymptotically. This means that the more samples we have, the more accurate would $ECE$'s estimate become. We can test this by generating *5x* as many samples as before and evaluating $ECE$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_samples = sampler.get_sample_arrays(5000)\n",
    "ground_truth, confidences = calibrated_samples\n",
    "\n",
    "ece.compute(confidences, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, $ECE$ goes down with more samples.\n",
    "\n",
    "We can also systematically generate uncalibrated samples. For instance, the `ShiftingSimplexAutomorphism` shifts the confidence scores by adding a fixed vector with positive entries to the input and normalizing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifting_vector = np.array([0, 2])\n",
    "automorphism = ShiftingSimplexAutomorphism(shifting_vector)\n",
    "shifted_sampler = DirichletFC(num_classes=2, simplex_automorphism=automorphism)\n",
    "\n",
    "# Get 1000 uncalibrated fake confidence scores\n",
    "uncalibrated_samples = shifted_sampler.get_sample_arrays(1000)\n",
    "ground_truth, confidences = uncalibrated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the uncalibrated nature of the samples is validated by $ECE$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece.compute(confidences, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to verify that miscalibration will indeed increase with more samples, let's sample *5x* as many samples as before and measure $ECE$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncalibrated_samples = shifted_sampler.get_sample_arrays(1000)\n",
    "ground_truth, confidences = uncalibrated_samples\n",
    "\n",
    "ece.compute(confidences, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Calibration error goes up as we sample more instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
